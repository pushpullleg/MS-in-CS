{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpLWzrUYoKTj"
      },
      "source": [
        "### Assignment3\n",
        "#### 50380788 Mukesh Ravichandran\n",
        "\n",
        "#### Data files for this work are \n",
        "* **Google.csv**, the Google Products dataset\n",
        "* **Amazon.csv**, the Amazon dataset\n",
        "* **Google_small.csv**, 200 records sampled from the Google data\n",
        "* **Amazon_small.csv**, 200 records sampled from the Amazon data\n",
        "* **Amazon_Google_perfectMapping.csv**, the \"gold standard\" mapping\n",
        "* **stopwords.txt**, a list of common English words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE19VIGrcUKW",
        "outputId": "f490d267-c868-4ee2-f3ff-49bedec666c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=e46420919207064dfc4a280e311b0ad86bdefad417f4ddf6c597c69a1265067c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120895 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "mZIs4E-u_X4b",
        "outputId": "78cc6790-fdc6-4536-d0f3-e3955000dbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=f4c15b78c7144580fadaac8a748d5278cd06bc28053fd6c6148ae79e8a758c00\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'stopwords.txt'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Just run this code\n",
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "GOOGLE_PATH = 'Google.csv'\n",
        "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
        "AMAZON_PATH = 'Amazon.csv'\n",
        "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
        "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
        "STOPWORDS_PATH = 'stopwords.txt'\n",
        "\n",
        "url = 'https://phantom.cs.qc.cuny.edu/li/bda/'\n",
        "wget.download(url + GOOGLE_PATH, GOOGLE_PATH)\n",
        "wget.download(url + GOOGLE_SMALL_PATH, GOOGLE_SMALL_PATH)\n",
        "wget.download(url + AMAZON_PATH, AMAZON_PATH)\n",
        "wget.download(url + AMAZON_SMALL_PATH, AMAZON_SMALL_PATH)\n",
        "wget.download(url + GOLD_STANDARD_PATH, GOLD_STANDARD_PATH)\n",
        "wget.download(url + STOPWORDS_PATH, STOPWORDS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvFq9dV2W25X"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "class TestFailure(Exception):\n",
        "  pass\n",
        "class PrivateTestFailure(Exception):\n",
        "  pass\n",
        "\n",
        "class Test(object):\n",
        "  passed = 0\n",
        "  numTests = 0\n",
        "  failFast = False\n",
        "  private = False\n",
        "\n",
        "  @classmethod\n",
        "  def setFailFast(cls):\n",
        "    cls.failFast = True\n",
        "\n",
        "  @classmethod\n",
        "  def setPrivateMode(cls):\n",
        "    cls.private = True\n",
        "\n",
        "  @classmethod\n",
        "  def assertTrue(cls, result, msg=\"\"):\n",
        "    cls.numTests += 1\n",
        "    if result == True:\n",
        "      cls.passed += 1\n",
        "      print(\"1 test passed.\")\n",
        "    else:\n",
        "      print(\"1 test failed. \" + msg)\n",
        "      if cls.failFast:\n",
        "        if cls.private:\n",
        "          raise PrivateTestFailure(msg)\n",
        "        else:\n",
        "          raise TestFailure(msg)\n",
        "\n",
        "  @classmethod\n",
        "  def assertEquals(cls, var, val, msg=\"\"):\n",
        "    cls.assertTrue(var == val, msg)\n",
        "\n",
        "  @classmethod\n",
        "  def assertEqualsHashed(cls, var, hashed_val, msg=\"\"):\n",
        "    cls.assertEquals(cls._hash(var), hashed_val, msg)\n",
        "\n",
        "  @classmethod\n",
        "  def printStats(cls):\n",
        "    print(\"{0} / {1} test(s) passed.\".format(cls.passed, cls.numTests))\n",
        "\n",
        "  @classmethod\n",
        "  def _hash(cls, x):\n",
        "    return hashlib.sha1(str(x)).hexdigest()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgMvkrhoKTk"
      },
      "source": [
        "### **Part 0: Preliminaries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfopkEJVoKTk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
        "\n",
        "def removeQuotes(s):\n",
        "    \"\"\" Remove quotation marks from an input string\n",
        "    Args:\n",
        "        s (str): input string that might have the quote \"\" characters\n",
        "    Returns:\n",
        "        str: a string without the quote characters\n",
        "    \"\"\"\n",
        "    return ''.join(i for i in s if i!='\"')\n",
        "\n",
        "\n",
        "def parseDatafileLine(datafileLine):\n",
        "    \"\"\" Parse a line of the data file using the specified regular expression pattern\n",
        "    Args:\n",
        "        datafileLine (str): input string that is a line from the data file\n",
        "    Returns:\n",
        "        str: a string parsed using the given regular expression and without the quote characters\n",
        "    \"\"\"\n",
        "    match = re.search(DATAFILE_PATTERN, datafileLine.decode('UTF-8'))\n",
        "    if match is None:\n",
        "        print('Invalid datafile line: %s' % datafileLine)\n",
        "        return (datafileLine, -1)\n",
        "    elif match.group(1) == '\"id\"':\n",
        "        print('Header datafile line: %s' % datafileLine)\n",
        "        return (datafileLine, 0)\n",
        "    else:\n",
        "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
        "        return ((removeQuotes(match.group(1)), product), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq53FWLvoKTl",
        "outputId": "99f62de9-2b9e-4b35-d04b-293a1d8d4be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\n",
            "Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines\n",
            "Amazon_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\n",
            "Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def parseData(filename):\n",
        "    \"\"\" Parse a data file\n",
        "    Args:\n",
        "        filename (str): input file name of the data file\n",
        "    Returns:\n",
        "        RDD: a RDD of parsed lines\n",
        "    \"\"\"\n",
        "    return (sc\n",
        "            .textFile(filename, 4, 0)\n",
        "            .map(parseDatafileLine)\n",
        "            .cache())\n",
        "\n",
        "def loadData(filename):\n",
        "    \"\"\" Load a data file\n",
        "    Args:\n",
        "        filename (str): input file name of the data file\n",
        "    Returns:\n",
        "        RDD: a RDD of parsed valid lines\n",
        "    \"\"\"\n",
        "    raw = parseData(filename).cache()\n",
        "    failed = (raw\n",
        "              .filter(lambda s: s[1] == -1)\n",
        "              .map(lambda s: s[0]))\n",
        "    for line in failed.take(10):\n",
        "        print('%s - Invalid datafile line: %s' % (path, line))\n",
        "    valid = (raw\n",
        "             .filter(lambda s: s[1] == 1)\n",
        "             .map(lambda s: s[0])\n",
        "             .cache())\n",
        "    print('%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (filename,\n",
        "                                                                                        raw.count(),\n",
        "                                                                                        valid.count(),\n",
        "                                                                                        failed.count()))\n",
        "    assert failed.count() == 0\n",
        "    assert raw.count() == (valid.count() + 1)\n",
        "    return valid\n",
        "\n",
        "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
        "google = loadData(GOOGLE_PATH)\n",
        "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
        "amazon = loadData(AMAZON_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvw-Q1cmoKTm"
      },
      "source": [
        "#### Let's examine the lines that were just loaded in the two subset (small) files - one from Google and one from Amazon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiQPor91oKTm",
        "outputId": "f0ef5861-8192-4201-cc52-4973b60e4e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "google: http://www.google.com/base/feeds/snippets/11448761432933644608: spanish vocabulary builder \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
            "\n",
            "google: http://www.google.com/base/feeds/snippets/8175198959985911471: topics presents: museums of world \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
            "\n",
            "google: http://www.google.com/base/feeds/snippets/18445827127704822533: sierrahome hse hallmark card studio special edition win 98 me 2000 xp \"hallmark card studio special edition (win 98 me 2000 xp)\" \"sierrahome\"\n",
            "\n",
            "amazon: b000jz4hqo: clickart 950 000 - premier image pack (dvd-rom)  \"broderbund\"\n",
            "\n",
            "amazon: b0006zf55o: ca international - arcserve lap/desktop oem 30pk \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
            "\n",
            "amazon: b00004tkvy: noah's ark activity center (jewel case ages 3-8)  \"victory multimedia\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for line in googleSmall.take(3):\n",
        "    print('google: %s: %s\\n' % (line[0], line[1]))\n",
        "\n",
        "for line in amazonSmall.take(3):\n",
        "    print('amazon: %s: %s\\n' % (line[0], line[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB7eqiC4oKTn"
      },
      "source": [
        "### **Part 1: ER as Text Similarity - Bags of Words**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHlY2t8QoKTn"
      },
      "source": [
        "### **1(a) Tokenize a String**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOByufzLoKTn",
        "outputId": "0de752f1-b608-400c-ee65-0a0beefb74b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
        "split_regex = r'\\W+'\n",
        "\n",
        "def simpleTokenize(string):\n",
        "    \"\"\" A simple implementation of input string tokenization\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens\n",
        "    \"\"\"\n",
        "    return [token for token in re.split(split_regex, string.lower()) if token != \"\"]\n",
        "\n",
        "print(simpleTokenize(quickbrownfox)) # Should give ['a', 'quick', 'brown', ... ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB9PepyioKTn",
        "outputId": "2ce35cff-a78e-4d05-830d-5fbf94eed2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Tokenize a String (1a)\n",
        "Test.assertEquals(simpleTokenize(quickbrownfox),\n",
        "                  ['a','quick','brown','fox','jumps','over','the','lazy','dog'],\n",
        "                  'simpleTokenize should handle sample text')\n",
        "Test.assertEquals(simpleTokenize(' '), [], 'simpleTokenize should handle empty string')\n",
        "Test.assertEquals(simpleTokenize('!!!!123A/456_B/789C.123A'), ['123a','456_b','789c','123a'],\n",
        "                  'simpleTokenize should handle puntuations and lowercase result')\n",
        "Test.assertEquals(simpleTokenize('fox fox'), ['fox', 'fox'],\n",
        "                  'simpleTokenize should not remove duplicates')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZBndm_VoKTn"
      },
      "source": [
        "### **(1b) Removing stopwords**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2bret_soKTo",
        "outputId": "fcbf0527-a6bc-459f-9991-a88bb6cb2143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "These are the stopwords: {'being', 'doing', 'her', 'he', 'over', 'nor', 'they', 'any', 'off', 'his', 'down', 'can', 'after', 'once', 'each', 'before', 'too', 'the', 'until', 'will', 'most', 'does', 'she', 'here', 'i', 'or', 'just', 'a', 'did', 'is', 'out', 'them', 'about', 'under', 'but', 'between', 'up', 'again', 'only', 'through', 'if', 'when', 'who', 'then', 'not', 'those', 'an', 'this', 'more', 'it', 'very', 'had', 't', 'has', 'my', 'both', 'further', 'theirs', 'these', 'we', 'herself', 'their', 'same', 'me', 'because', 'where', 'during', 'you', 'such', 'myself', 'your', 'now', 'don', 'all', 'ourselves', 'against', 'yourselves', 'from', 'himself', 'yourself', 'having', 'into', 'was', 'themselves', 'our', 'him', 'itself', 'so', 'are', 'on', 'how', 'no', 'at', 'of', 'than', 'be', 'in', 'its', 'while', 'been', 'as', 'there', 'by', 'have', 'few', 'do', 'for', 'some', 'whom', 'with', 'ours', 'were', 'yours', 'and', 'which', 'am', 'what', 'that', 'why', 'should', 'hers', 'to', 'below', 's', 'above', 'other', 'own'}\n",
            "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "stopfile = STOPWORDS_PATH\n",
        "stopwords = set(sc.textFile(stopfile).collect())\n",
        "print('These are the stopwords: %s' % stopwords)\n",
        "\n",
        "def tokenize(string):\n",
        "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    return [word for word in simpleTokenize(string) if not word in stopwords]\n",
        "\n",
        "print(tokenize(quickbrownfox)) # Should give ['quick', 'brown', ... ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Cflr6ToKTo",
        "outputId": "5a0ea7d2-4ef5-408f-8a4b-7cb8edb170c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Removing stopwords (1b)\n",
        "Test.assertEquals(tokenize(\"Why a the?\"), [], 'tokenize should remove all stopwords')\n",
        "Test.assertEquals(tokenize(\"Being at the_?\"), ['the_'], 'tokenize should handle non-stopwords')\n",
        "Test.assertEquals(tokenize(quickbrownfox), ['quick','brown','fox','jumps','lazy','dog'],\n",
        "                    'tokenize should handle sample text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVToH5eyoKTo"
      },
      "source": [
        "### **(1c) Tokenizing the small datasets**\n",
        "#### Now let's tokenize the two *small* datasets. For each ID in a dataset, `tokenize` the values, and then count the total number of tokens.\n",
        "#### How many tokens, total, are there in the two datasets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEzsYi4UoKTp",
        "outputId": "1a5376fe-2340-4578-b2a2-cd38d2747dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 22520 tokens in the combined datasets\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "amazonRecToToken = amazonSmall.map(lambda name_product: (name_product[0], tokenize(name_product[1])))\n",
        "googleRecToToken = googleSmall.map(lambda name_product: (name_product[0], tokenize(name_product[1])))\n",
        "\n",
        "def countTokens(vendorRDD):\n",
        "    \"\"\" Count and return the number of tokens\n",
        "    Args:\n",
        "        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output\n",
        "    Returns:\n",
        "        count: count of all tokens\n",
        "    \"\"\"\n",
        "    return vendorRDD.map(lambda name_tokens: len(name_tokens[1])).reduce(lambda a, b: a + b)\n",
        "\n",
        "totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)\n",
        "print('There are %s tokens in the combined datasets' % totalTokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dsG6y_6oKTp"
      },
      "source": [
        "### **(1d) Amazon record with the most tokens**\n",
        "#### Which Amazon record has the biggest number of tokens?\n",
        "#### In other words, you want to sort the records and get the one with the largest count of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17l35bhPoKTq",
        "outputId": "24dab6a5-02c0-4730-abb4-5030d56262e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Amazon record with ID \"b000o24l3q\" has the most tokens (1547)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def findBiggestRecord(vendorRDD):\n",
        "    \"\"\" Find and return the record with the largest number of tokens\n",
        "    Args:\n",
        "        vendorRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens\n",
        "    Returns:\n",
        "        list: a list of 1 Pair Tuple of record ID and tokens\n",
        "    \"\"\"\n",
        "    return vendorRDD.takeOrdered(1, lambda name_tokens: -1 * len(name_tokens[1]))\n",
        "\n",
        "biggestRecordAmazon = findBiggestRecord(amazonRecToToken)\n",
        "print('The Amazon record with ID \"%s\" has the most tokens (%s)' % (biggestRecordAmazon[0][0],\n",
        "                                                                   len(biggestRecordAmazon[0][1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUHESnhaoKTq"
      },
      "source": [
        "### **Part 2: ER as Text Similarity - Weighted Bag-of-Words using TF-IDF**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B5fg66ToKTr"
      },
      "source": [
        "### **(2a) Implement a TF function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVHBv0pUoKTr",
        "outputId": "5fd83ca9-b958-4f4d-d81e-fa51bd99ed19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'quick': 0.16666666666666666, 'brown': 0.16666666666666666, 'fox': 0.16666666666666666, 'jumps': 0.16666666666666666, 'lazy': 0.16666666666666666, 'dog': 0.16666666666666666}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def tf(tokens):\n",
        "    \"\"\" Compute TF\n",
        "    Args:\n",
        "        tokens (list of str): input list of tokens from tokenize\n",
        "    Returns:\n",
        "        dictionary: a dictionary of tokens to its TF values\n",
        "    \"\"\"\n",
        "    dictionary = {}\n",
        "    for token in tokens:\n",
        "        if (token in dictionary):\n",
        "            dictionary[token] += 1\n",
        "        else:\n",
        "            dictionary[token] = 1\n",
        "    for token, frequency in dictionary.items():\n",
        "        dictionary[token] = float(frequency) / float(len(tokens))\n",
        "    return dictionary\n",
        "\n",
        "print(tf(tokenize(quickbrownfox))) # Should give { 'quick': 0.1666 ... }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_hkfJ-3oKTr",
        "outputId": "ee6dd283-ceb1-4a49-e5ef-71b7bc0c2c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Implement a TF function (2a)\n",
        "tf_test = tf(tokenize(quickbrownfox))\n",
        "Test.assertEquals(tf_test, {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,\n",
        "                             'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,\n",
        "                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666},\n",
        "                    'incorrect result for tf on sample text')\n",
        "tf_test2 = tf(tokenize('one_ one_ two!'))\n",
        "Test.assertEquals(tf_test2, {'one_': 0.6666666666666666, 'two': 0.3333333333333333},\n",
        "                    'incorrect result for tf test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l12xO0fhoKTr"
      },
      "source": [
        "### **(2b) Create a corpus**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxZmzrkcoKTr"
      },
      "outputs": [],
      "source": [
        "\n",
        "corpusRDD = amazonRecToToken.union(googleRecToToken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO1YfTqHoKTs",
        "outputId": "b633197d-198a-4bf5-d96a-65be9ce3d6e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Create a corpus (2b)\n",
        "Test.assertEquals(corpusRDD.count(), 400, 'incorrect corpusRDD.count()')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786b-NFloKTs"
      },
      "source": [
        "### **(2c) Implement an IDFs function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kyyHbReEoKTs",
        "outputId": "3c9ceabf-98ac-48c6-c26a-762e49ad1d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 4772 unique tokens in the small datasets.\n"
          ]
        }
      ],
      "source": [
        "def idfs(corpus):\n",
        "    \"\"\" Compute IDF\n",
        "    Args:\n",
        "        corpus (RDD): input corpus\n",
        "    Returns:\n",
        "        RDD: a RDD of (record ID, IDF value)\n",
        "    \"\"\"\n",
        "    N = corpus.count()\n",
        "    uniqueTokens = corpus.flatMap(lambda name_tokens: name_tokens[1]).distinct()\n",
        "    tokenCountPairTuple = uniqueTokens.cartesian(corpus)\n",
        "    tokenSumPairTuple = tokenCountPairTuple.map(lambda token_name_tokens: (token_name_tokens[0], 1) if token_name_tokens[0] in token_name_tokens[1][1] else (token_name_tokens[0], 0)).reduceByKey(lambda a, b: a + b)\n",
        "    return (tokenSumPairTuple.map(lambda token_frequency: (token_frequency[0], float(N) / float(token_frequency[1]))))\n",
        "\n",
        "idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))\n",
        "uniqueTokenCount = idfsSmall.count()\n",
        "\n",
        "print('There are %s unique tokens in the small datasets.' % uniqueTokenCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OZ58l8gpoKTs",
        "outputId": "f50b7c6f-9dd6-42dd-8999-c7035a544ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Implement an IDFs function (2c)\n",
        "Test.assertEquals(uniqueTokenCount, 4772, 'incorrect uniqueTokenCount')\n",
        "tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]\n",
        "Test.assertEquals(tokenSmallestIdf[0], 'software', 'incorrect smallest IDF token')\n",
        "Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894 < 0.0000000001),\n",
        "                'incorrect smallest IDF value')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myrRPsVIoKTs"
      },
      "source": [
        "### **(2d) Tokens with the smallest IDF**\n",
        "#### Print out the 11 tokens with the smallest IDF in the combined small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KrYU__5voKTs",
        "outputId": "b9fc63d5-9649-4d8d-ae72-c2e13ed3a33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('software', 4.25531914893617), ('new', 6.896551724137931), ('features', 6.896551724137931), ('use', 7.017543859649122), ('complete', 7.2727272727272725), ('easy', 7.6923076923076925), ('create', 8.333333333333334), ('system', 8.333333333333334), ('cd', 8.333333333333334), ('1', 8.51063829787234), ('windows', 8.51063829787234)]\n"
          ]
        }
      ],
      "source": [
        "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
        "print(smallIDFTokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZS65vusoKTt"
      },
      "source": [
        "### **(2e) IDF Histogram**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VlMmEG0koKTt",
        "outputId": "4ab1d161-98b8-41b4-c331-57454489981f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
        "fig = plt.figure(figsize=(8,3))\n",
        "plt.hist(small_idf_values, 50, log=True)\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5d7HMQBoKTt"
      },
      "source": [
        "### **(2f) Implement a TF-IDF function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fFyIiucHoKTt",
        "outputId": "03bc96cf-61f3-48ae-9c76-f20eabb6f3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amazon record \"b000hkgj8k\" has tokens and weights:\n",
            "{'autocad': 33.33333333333333, '2007': 3.5087719298245617, 'courseware': 66.66666666666666, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303, 'autodesk': 8.333333333333332, 'psg': 33.33333333333333}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def tfidf(tokens, idfs):\n",
        "    \"\"\" Compute TF-IDF\n",
        "    Args:\n",
        "        tokens (list of str): input list of tokens from tokenize\n",
        "        idfs (dictionary): record to IDF value\n",
        "    Returns:\n",
        "        dictionary: a dictionary of records to TF-IDF values\n",
        "    \"\"\"\n",
        "    tfs = tf(tokens)\n",
        "    tfIdfDict = dict(map(lambda kv: (kv[0], kv[1] * idfs[kv[0]]), tfs.items()))\n",
        "    return tfIdfDict\n",
        "\n",
        "recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
        "idfsSmallWeights = idfsSmall.collectAsMap()\n",
        "recb000hkgj8kWeights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
        "\n",
        "rec_b000hkgj8k_weights = recb000hkgj8kWeights\n",
        "print('Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GBE4-NwRoKTu",
        "outputId": "b13fde18-7f32-4669-8a44-4fe8b6506e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Implement a TF-IDF function (2f)\n",
        "Test.assertEquals(rec_b000hkgj8k_weights,\n",
        "                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,\n",
        "                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,\n",
        "                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,\n",
        "                    'interface': 3.0303030303030303}, 'incorrect rec_b000hkgj8k_weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiAhzjUSoKTu"
      },
      "source": [
        "### **Part 3: ER as Text Similarity - Cosine Similarity**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpygTaJ6oKTu"
      },
      "source": [
        "### **(3a) Implement the components of a `cosineSimilarity` function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AEUxcs0boKTu",
        "outputId": "7b734ee8-d697-43b0-f2c6-0a2b5c227ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102 6.164414002968976\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def dotprod(a, b):\n",
        "    \"\"\" Compute dot product\n",
        "    Args:\n",
        "        a (dictionary): first dictionary of record to value\n",
        "        b (dictionary): second dictionary of record to value\n",
        "    Returns:\n",
        "        dotProd: result of the dot product with the two input dictionaries\n",
        "    \"\"\"\n",
        "    return sum([value * b[key] for key, value in a.items() if key in b])\n",
        "\n",
        "def norm(a):\n",
        "    \"\"\" Compute square root of the dot product\n",
        "    Args:\n",
        "        a (dictionary): a dictionary of record to value\n",
        "    Returns:\n",
        "        norm: a dictionary of tokens to its TF values\n",
        "    \"\"\"\n",
        "    return math.sqrt(dotprod(a, a))\n",
        "\n",
        "def cossim(a, b):\n",
        "    \"\"\" Compute cosine similarity\n",
        "    Args:\n",
        "        a (dictionary): first dictionary of record to value\n",
        "        b (dictionary): second dictionary of record to value\n",
        "    Returns:\n",
        "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
        "                then by the norm of the second dictionary\n",
        "    \"\"\"\n",
        "    return dotprod(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
        "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
        "dp = dotprod(testVec1, testVec2)\n",
        "nm = norm(testVec1)\n",
        "print(dp, nm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_KxSyVcJoKTu",
        "outputId": "186a236e-602b-43b8-dce5-a65b130d95c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Implement the components of a cosineSimilarity function (3a)\n",
        "Test.assertEquals(dp, 102, 'incorrect dp')\n",
        "Test.assertTrue(abs(nm - 6.16441400297) < 0.0000001, 'incorrrect nm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cwROzawoKTu"
      },
      "source": [
        "### **(3b) Implement a `cosineSimilarity` function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dwo0nrxZoKTv",
        "outputId": "8a04c96c-c27b-4cf8-a3c2-c3cbfe6e1bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.05772433821630337\n"
          ]
        }
      ],
      "source": [
        "def cosineSimilarity(string1, string2, idfsDictionary):\n",
        "    \"\"\" Compute cosine similarity between two strings\n",
        "    Args:\n",
        "        string1 (str): first string\n",
        "        string2 (str): second string\n",
        "        idfsDictionary (dictionary): a dictionary of IDF values\n",
        "    Returns:\n",
        "        cossim: cosine similarity value\n",
        "    \"\"\"\n",
        "    w1 = tfidf(tokenize(string1), idfsDictionary)\n",
        "    w2 = tfidf(tokenize(string2), idfsDictionary)\n",
        "    return cossim(w1, w2)\n",
        "\n",
        "cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
        "                               'Adobe Illustrator',\n",
        "                               idfsSmallWeights)\n",
        "\n",
        "print(cossimAdobe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f_5XKCwDoKTv",
        "outputId": "62a8f504-4f3a-457f-8045-ebbc33713cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Implement a cosineSimilarity function (3b)\n",
        "Test.assertTrue(abs(cossimAdobe - 0.0577243382163) < 0.0000001, 'incorrect cossimAdobe')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639cMn-foKTv"
      },
      "source": [
        "### **(3c) Perform Entity Resolution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j6-dckt8oKTv",
        "outputId": "3500e462-4517-4183-d77f-2339f743bf64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requested similarity is 0.00030317194045132.\n"
          ]
        }
      ],
      "source": [
        "crossSmall = (googleSmall\n",
        "              .cartesian(amazonSmall)\n",
        "              .cache())\n",
        "idfs_small_weights = idfsSmallWeights\n",
        "\n",
        "def computeSimilarity(record):\n",
        "    \"\"\" Compute similarity on a combination record\n",
        "    Args:\n",
        "        record: a pair, (google record, amazon record)\n",
        "    Returns:\n",
        "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
        "    \"\"\"\n",
        "    googleRec = record[0]\n",
        "    amazonRec = record[1]\n",
        "    googleURL = googleRec[0]\n",
        "    amazonID = amazonRec[0]\n",
        "    googleValue = googleRec[1]\n",
        "    amazonValue = amazonRec[1]\n",
        "    cs = cosineSimilarity(googleValue, amazonValue, idfs_small_weights)\n",
        "    return (googleURL, amazonID, cs)\n",
        "\n",
        "similarities = (crossSmall\n",
        "                .map(lambda record: computeSimilarity(record))\n",
        "                .cache())\n",
        "\n",
        "def similar(amazonID, googleURL):\n",
        "    \"\"\" Return similarity value\n",
        "    Args:\n",
        "        amazonID: amazon ID\n",
        "        googleURL: google URL\n",
        "    Returns:\n",
        "        similar: cosine similarity value\n",
        "    \"\"\"\n",
        "    return (similarities\n",
        "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
        "            .collect()[0][2])\n",
        "\n",
        "similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
        "print('Requested similarity is %s.' % similarityAmazonGoogle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I1mqZCDmoKTv",
        "outputId": "ad5aa6dd-8a60-41c6-8e2a-6b94893cab62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Perform Entity Resolution (3c)\n",
        "Test.assertTrue(abs(similarityAmazonGoogle - 0.000303171940451) < 0.0000001,\n",
        "                'incorrect similarityAmazonGoogle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZJErv5oKTw"
      },
      "source": [
        "### **(3d) Perform Entity Resolution with Broadcast Variables**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ynU69aTQoKTw",
        "outputId": "975357c7-e767-4415-fe8c-e563c16c0fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requested similarity is 0.00030317194045132.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def computeSimilarityBroadcast(record):\n",
        "    \"\"\" Compute similarity on a combination record, using Broadcast variable\n",
        "    Args:\n",
        "        record: a pair, (google record, amazon record)\n",
        "    Returns:\n",
        "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
        "    \"\"\"\n",
        "    googleRec = record[0]\n",
        "    amazonRec = record[1]\n",
        "    googleURL = googleRec[0]\n",
        "    amazonID = amazonRec[0]\n",
        "    googleValue = googleRec[1]\n",
        "    amazonValue = amazonRec[1]\n",
        "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallBroadcast.value)\n",
        "    return (googleURL, amazonID, cs)\n",
        "\n",
        "idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)\n",
        "similaritiesBroadcast = (crossSmall\n",
        "                         .map(lambda record: computeSimilarityBroadcast(record))\n",
        "                         .cache())\n",
        "\n",
        "def similarBroadcast(amazonID, googleURL):\n",
        "    \"\"\" Return similarity value, computed using Broadcast variable\n",
        "    Args:\n",
        "        amazonID: amazon ID\n",
        "        googleURL: google URL\n",
        "    Returns:\n",
        "        similar: cosine similarity value\n",
        "    \"\"\"\n",
        "    return (similaritiesBroadcast\n",
        "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
        "            .collect()[0][2])\n",
        "\n",
        "similarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
        "print('Requested similarity is %s.' % similarityAmazonGoogleBroadcast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "39u4XUOboKTw",
        "outputId": "1e336780-de8f-4422-c8df-a4c0834fd7aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Perform Entity Resolution with Broadcast Variables (3d)\n",
        "from pyspark import Broadcast\n",
        "Test.assertTrue(isinstance(idfsSmallBroadcast, Broadcast), 'incorrect idfsSmallBroadcast')\n",
        "Test.assertEquals(len(idfsSmallBroadcast.value), 4772, 'incorrect idfsSmallBroadcast value')\n",
        "Test.assertTrue(abs(similarityAmazonGoogleBroadcast - 0.000303171940451) < 0.0000001,\n",
        "                'incorrect similarityAmazonGoogle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUJg-L2joKTw"
      },
      "source": [
        "### **(3e) Perform a Gold Standard evaluation**\n",
        "#### First, we'll load the \"gold standard\" data and use it to answer several questions. We read and parse the Gold Standard data, where the format of each line is \"Amazon Product ID\",\"Google URL\". The resulting RDD has elements of the form (\"AmazonID GoogleURL\", 'gold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AjFPS2ZWoKTw",
        "outputId": "d54a80d9-3eee-44f8-d888-9da81de52802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 1301 lines, successfully parsed 1300 lines, failed to parse 0 lines\n"
          ]
        }
      ],
      "source": [
        "GOLDFILE_PATTERN = '^(.+),(.+)'\n",
        "\n",
        "# Parse each line of a data file useing the specified regular expression pattern\n",
        "def parse_goldfile_line(goldfile_line):\n",
        "    \"\"\" Parse a line from the 'golden standard' data file\n",
        "    Args:\n",
        "        goldfile_line: a line of data\n",
        "    Returns:\n",
        "        pair: ((key, 'gold', 1 if successful or else 0))\n",
        "    \"\"\"\n",
        "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
        "    if match is None:\n",
        "        print('Invalid goldfile line: %s' % goldfile_line)\n",
        "        return (goldfile_line, -1)\n",
        "    elif match.group(1) == '\"idAmazon\"':\n",
        "        print('Header datafile line: %s' % goldfile_line)\n",
        "        return (goldfile_line, 0)\n",
        "    else:\n",
        "        key = '%s %s' % (removeQuotes(match.group(1)), removeQuotes(match.group(2)))\n",
        "        return ((key, 'gold'), 1)\n",
        "\n",
        "goldfile = GOLD_STANDARD_PATH\n",
        "gsRaw = (sc\n",
        "         .textFile(goldfile)\n",
        "         .map(parse_goldfile_line)\n",
        "         .cache())\n",
        "\n",
        "gsFailed = (gsRaw\n",
        "            .filter(lambda s: s[1] == -1)\n",
        "            .map(lambda s: s[0]))\n",
        "for line in gsFailed.take(10):\n",
        "    print('Invalid goldfile line: %s' % line)\n",
        "\n",
        "goldStandard = (gsRaw\n",
        "                .filter(lambda s: s[1] == 1)\n",
        "                .map(lambda s: s[0])\n",
        "                .cache())\n",
        "\n",
        "print('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (gsRaw.count(),\n",
        "                                                                                 goldStandard.count(),\n",
        "                                                                                 gsFailed.count()))\n",
        "assert (gsFailed.count() == 0)\n",
        "assert (gsRaw.count() == (goldStandard.count() + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyd52O7loKTx"
      },
      "source": [
        "\n",
        "* #### How many true duplicate pairs are there in the small datasets?\n",
        "* #### What is the average similarity score for true duplicates?\n",
        "* #### What about for non-duplicates?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YKUtVdDjoKTx",
        "outputId": "8c368b4b-3b15-4c75-b70d-5ddec3fb48f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 146 true duplicates.\n",
            "The average similarity of true duplicates is 0.26433257343519145.\n",
            "And for non duplicates, it is 0.0012347630465555245.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sims = similaritiesBroadcast.map(lambda googleURL_amazonID_cs: (\"{0} {1}\".format(googleURL_amazonID_cs[1], googleURL_amazonID_cs[0]), googleURL_amazonID_cs[2]))\n",
        "\n",
        "trueDupsRDD = (sims\n",
        "               .join(goldStandard))\n",
        "trueDupsCount = trueDupsRDD.count()\n",
        "avgSimDups = trueDupsRDD.map(lambda key_value: key_value[1][0]).reduce(lambda a, b: a + b) / float(trueDupsCount)\n",
        "\n",
        "nonDupsRDD = (sims\n",
        "              .leftOuterJoin(goldStandard)\n",
        "              .filter(lambda key_value: key_value[1][1] is None)\n",
        "              .map(lambda key_value: key_value[1][0]))\n",
        "avgSimNon = nonDupsRDD.reduce(lambda a, b: a + b) / float(nonDupsRDD.count())\n",
        "\n",
        "print('There are %s true duplicates.' % trueDupsCount)\n",
        "print('The average similarity of true duplicates is %s.' % avgSimDups)\n",
        "print('And for non duplicates, it is %s.' % avgSimNon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yKtHHCksoKTx",
        "outputId": "987bb803-65c4-4a9d-f92a-d284f2d1587b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 test passed.\n",
            "1 test passed.\n",
            "1 test passed.\n"
          ]
        }
      ],
      "source": [
        "# TEST Perform a Gold Standard evaluation (3e)\n",
        "Test.assertEquals(trueDupsCount, 146, 'incorrect trueDupsCount')\n",
        "Test.assertTrue(abs(avgSimDups - 0.264332573435) < 0.0000001, 'incorrect avgSimDups')\n",
        "Test.assertTrue(abs(avgSimNon - 0.00123476304656) < 0.0000001, 'incorrect avgSimNon')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fUHESnhaoKTq",
        "hiAhzjUSoKTu",
        "Nae0o_0XoKTx"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
